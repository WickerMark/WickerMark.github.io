[
  {
    "objectID": "posts/primes.html",
    "href": "posts/primes.html",
    "title": "Generating the nth Prime Number",
    "section": "",
    "text": "Numpy will be used for the square root function later in the code.\nAn array used to store the prime numbers is created and the first two prime numbers are stored.\nNumber, the variable used to represent the current number to be checked if it is prime, is set to 3. This means that the counter, used to represent how many prime numbers have been found, is set to 2, since 3 is the second prime number.\n\nimport numpy as np\n\nprime_list = [2, 3]\nnumber = 3\ncounter = 2\n\nThe prime number to be generated can be set to anything, but for this example, I have used 1000. This means that the while loop will continue to iterate until the counter is no longer less than 1000.\nThe variable number is first incremented by 2 (to emit all even numbers since these cannot be prime).\nA new variable, called prime, stores a boolean value which states if a number is prime or not. Initially, prime is set to True.\nA for loop is used to check if number is divisible by previous prime numbers.\nThe square root function is used here because if the index of the prime number to be checked is greater than the square root of the number, it is not a factor. If this statement is true, the code breaks out of the for loop.\nHere, I use the modulo operator to determine if the number has a remainder when dividing by previous prime numbers. If it does, the number is determined as a composite number, and so prime is set to False.\nThe final if statement is used such that if the number was determined to be prime, the counter would be incremented, and the number would be appended to the list of prime numbers so that future numbers can be checked against it.\nFinally, counter is cast as a string and concatenated to output the nth prime number.\n\nwhile counter &lt; 1000:\n    number += 2\n    prime = True\n    for i in prime_list:\n        if i &gt; np.sqrt(number):\n            break\n        elif number % i == 0:\n            prime = False\n            break\n    \n    if prime == True:\n        counter += 1\n        prime_list.append(number)\nprint(\"The\", str(counter) + \"th prime number is\", number)\n\nThe 1000th prime number is 7919"
  },
  {
    "objectID": "posts/derivative.html",
    "href": "posts/derivative.html",
    "title": "A Numerical Method to Calculate the Derivative using a Small Step, ϵ",
    "section": "",
    "text": "The algorithm created is based on the equation \\(f'(x)\\approx\\frac{f(x + \\epsilon)-f(x - \\epsilon)}{2\\epsilon}\\) to approximate the derivative using a small step. I have set the small step to \\(\\epsilon=1\\times10^{-8}\\), however, this can be modified to the specification of the user.\nTo begin the algorithm, the required libraries are imported, specifically numpy for trigonometric and logarithmic functions and matplotlib for the plotting.\nNext, the function to be differentiated is defined. For the purpose of this example, the function used is \\(f(x)=\\ln(x^2)\\sin(x)\\). The user can change the function to whatever may be required.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Creates the function\ndef f(x):\n    return np.log(x**2) * np.sin(x)\n\nThis function finds the derivative using the small step, ϵ, at a given x coordinate. This is useful for plotting, because the desired array of x coordinates can be passed to the function to generate an f’(x) function for that set of values. For other algorithms, the gradient descent algorithm for instance, finding the derivative at a given x coordinate is useful for further calculations.\n\n# Function to differentiate\ndef derivative(f, x):\n    eps = 1e-8\n    der = (f(x + eps) - f(x - eps)) / (2 * eps)\n    \n    return der\n\nHere, np.linspace(-15, 15, 500) is used to create an equally spaced array of 500 x coordinates between x = -15 and x = 15. Then, for each x coordinate, a corresponding y coordinate is found by passing each x coordinate into the function.\n\n# Creates an array of x coordinates and finds corresponding y coordinates \n# for plotting\nx = np.linspace(-15, 15, 500)\ny = f(x)\n\nNow, list comprehension is used to apply the derivative function to each element \\(x_i\\) in \\(x\\). The result is then stored as a numpy array for plotting.\n\n# Differentiates x coordinates to create the graph of the first derivative\ndydx = np.array([derivative(f, xi) for xi in x])\n\nLastly, the curves f(x) and f’(x) are plotted and given appropriate labels.\nThen, the paramaters of the plot are set, the limit for the x axis, the creation of a grid and also the legend to display labels.\nThese graphs are then displayed.\n\n# Plots the curves\nplt.plot(x, y, label=\"f(x)\")\nplt.plot(x, dydx, label=\"f'(x)\")\n\n# Sets parameters of the plot and plots the curves\nplt.xlim(-15, 15)\nplt.grid()\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nThis algorithm has been curated for use in the gradient descent algorithm, which will be a future post."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Coding Projects",
    "section": "",
    "text": "A Numerical Method to Calculate the Derivative using a Small Step, ϵ\n\n\n\n\n\n\nPython\n\n\nNumerical Methods\n\n\n\nAn implementation of calculating the numerical derivative using the small step, ϵ, in Python, for the purpose of plotting and use in the gradient descent algorithm.\n\n\n\n\n\nJul 28, 2024\n\n\nMark Watson\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating the nth Prime Number\n\n\n\n\n\n\nPython\n\n\nPrime Numbers\n\n\n\nPython program which calculates an inputted nth prime number by dividing each consecutive number by previous prime numbers.\n\n\n\n\n\nAug 14, 2024\n\n\nMark\n\n\n\n\n\n\n\n\n\n\n\n\nOptimisation: The Gradient Descent Algorithm for Maching Learning\n\n\n\n\n\n\nPython\n\n\nOptimisation\n\n\nMachine Learning\n\n\n\nPython program which uses the gradient descent algorithm to calculate minimum points for a given function. Used for optimisation in machine learning.\n\n\n\n\n\nAug 13, 2024\n\n\nMark Watson\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About me"
  },
  {
    "objectID": "posts/optimisation.html",
    "href": "posts/optimisation.html",
    "title": "Optimisation: The Gradient Descent Algorithm for Maching Learning",
    "section": "",
    "text": "The objective of this blog post is to create an implementation of a one-dimensional gradient descent algorithm used for optimisation in machine learning where a function needs to be minimised. Shown below is the code and explanations of my implementation of the algorithm.\nFirst, the required libraries are imported and the function f(x) is defined for future use. This can be changed to any function that the user would require. In this example, I have used the equation \\(f(x)=x\\sin(x)+\\cos(x)+x\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Creates the function\ndef f(x):\n    return x * np.sin(x) + (np.cos(x) + x)\n\nThis function calculates the numerical derivative using the small step ϵ. This is defined by the equation \\(f'(x)\\approx\\frac{f(x + \\epsilon)-f(x - \\epsilon)}{2\\epsilon}\\).\n\n# Function to differentiate\ndef derivative(f, x):\n    eps = 1e-8\n    der = (f(x + eps) - f(x - eps)) / (2 * eps)\n    \n    return der\n\nThe following function performs the gradient descent algorithm. There are two stopping conditions for the algorithm, a maximum number of iterations, and if the absolute value of the gradient is lower than a set threshold value. The variable alpha is the learning rate of the algorithm, a scalar quantity which controls the size of the steps the algorithm takes. The equation \\(\\theta_{t+1}=\\theta_t-\\alpha\\nabla_{\\theta}f(\\theta_t)\\) is used to update the position of the marker until the minimum point is found.\n\n# Function to perform the gradient descent algorithm\ndef descent(f, derivative, xk, alpha=1e-3, tol=1e-4, max_iters=10000):\n    for i in range(max_iters):\n        grad = derivative(f, xk)\n        if abs(grad) &lt; tol:\n            break\n        xk = xk - alpha * grad\n\n    return xk\n\nThe algorithm is then ran from various different starting points until the lowest minima is found. Empty lits of x and y coordinates are created and appended to with the coordinates determined by the algorithm. In this example, I used starting points between -12 and 12. The y coordinate of the optimal minimum point is then determined and printed.\n\nxk_points = []\nyk_points = []\nfor i in np.arange(-12,12):\n    # Starting position\n    xk = i\n    # Finds the x coordinate of each minima\n    xk = descent(f, derivative, xk)\n    xk_points.append(xk)\n\n    # Finds the y coordinate of the minima for plotting\n    yk = f(xk)\n    yk_points.append(yk)\n\n# Prints the y coordiante of the optimal minimum\nprint(\"The optimal minimum has y coordinate\", min(yk_points))\n\nThe optimal minimum has y coordinate -22.036404343345218\n\n\nThe below code generates the function, its derivative and the minima found by the algorithm.\n\n# Creates an array of x coordinates and finds corresponding y coordinates \n# for plotting\nx = np.linspace(-15, 15, 100)\ny = f(x)\n\n# Differentiates x coordinates to create the graph of the first derivative\ndydx = np.array([derivative(f, xi) for xi in x])\n\n# Plots the curves\nplt.plot(x, y, label=\"f(x)\")\nplt.plot(x, dydx, label=\"f'(x)\")\nplt.scatter(xk_points, yk_points, marker=\"x\", color=\"red\", label=\"Minima\")\n\n# Sets parameters of the plot and plots the curves\nplt.xlim(-15, 15)\nplt.grid()\nplt.legend()\n\nplt.show()"
  }
]